> A workshop proposal for Pycon 2024

When you try to learn concurrency from tutorials that claim to be “introductory,” does it seem like you’re already supposed to know a lot? Do the terms appear to mean different things depending on the article you’re reading? Do you get frustrated when you hear someone declare that “concurrency is easy?”

It’s not easy—it’s a minefield of different concepts, terminology, and curse-of-knowledge explanations. It’s a Catch-22: you can’t begin to understand concurrency until you know enough of the foundational principles to successfully navigate introductory tutorials.

This workshop gives you those foundational concepts. It creates the basic understanding framework to enable you to read about concurrency and grasp what the authors are saying, regardless of the terminology they use or the specific implementation they may be describing.

It uses Python for examples, but the concepts are transferrable to other languages. The workshop assumes you are an intermediate Python programmer so you understand keywords and concepts, but you don’t have any background in concurrency.

- Concurrency is a leaky abstraction. Often you need to know operating system and hardware details to understand aspects.

## 1. Why are we doing this?
The only reason to consider concurrency is performance: something about your program isn’t producing results fast enough to meet your needs. This includes not only raw speed of data processing, but also the responsiveness of UI programs. Concurrency is only one strategy for solving performance problems, and it’s not necessarily the appropriate one, depending on your situation.
## 2. Performance alternatives
Performance tuning adds complexity to your program, which impacts the cost of creation and maintenance. You want to use the simplest approach you can to minimize these impacts. Before jumping into concurrency, consider other approaches to performance, as you might solve your problem more easily and maintainably using a different solution.
- By profiling your code you can find the places where it spends most of its time.
- You might be able to rewrite sections to improve performance
- You might be able to incorporate a library that fixes your bottleneck 
- If that doesn’t work, you can create an external module in a more performant language like Rust
## 3. What is concurrency?
- Breaking programs into pieces (tasks)
- Time-slicing those tasks (scheduling)
- The context switch
- The tasks must somehow improve performance through scheduling—but there’s no guarantee that they will.
- CPU bound problems and embarrassing parallelism.
- IO: anything that relies on an off-chip system to produce a result. Because you are talking to another system, there are numerous factors: You don’t know how long it will take, what kind of failures might happen, and whether it will even respond to your request. IO bound and tasks waiting on other tasks.
## 4. How do programs run?
Assuming you’re not doing embedded programming, you are running on an operating system. All programs start with a process and one thread to drive that program forward. You need to understand this structure, as well as the reason for the Python Global Interpreter Lock (GIL), to properly imagine what’s going on and how to make choices, even if you’re only using coroutines (there’s always a thread under there, running your code).
## 5. What are threads good for?
Many languages allow you to run multiple threads simultaneously, which can increase performance by making use of multiple cores. Python supports multiple threads, but the GIL prevents these from executing simultaneously—so why do we even have them?  You'll learn that, even with the advent of coroutines, threads still solve an important problem. In addition, threads have a problematic legacy.
## 6. Coroutines
- Suspension points by design rather than accident.
- The benefits of not saving the world:
- Context-switching size and speed
- Cancellation
- Error reporting
- The problem with coroutines: the library rewrite
## 7. Structured Concurrency
- What and why
- Python’s support for SC
## 8. Sharing Data
Sharing data between tasks is like sharing a lawnmower among neighbors. In some ways it’s more efficient, but you have to coordinate and deal with damage. We’ll look at some basic approaches for mitigating these issues..
## 9. Architectures
One of the most confusing aspects of concurrency is the numerous different kinds of problems that arise, and the different approaches for solving these problems. People commonly assume that the first approach they learn defines concurrency, and that becomes their worldview and point of reference when they talk about it, using the specific terminology and ideas of that approach.

It’s important to understand that concurrency is a collection of strategies.



From [TREE-STRUCTURED CONCURRENCY](https://blog.yoshuawuyts.com/tree-structured-concurrency/)
- Under structured concurrency every task has a parent, cancellation flows downward, and errors flow upward. Completion guarantees no dangling tasks. 
- "A function will return when it's done, will cancel all work when you ask it to, and you'll always receive an error if there is something which needs handling. And as a result code under this model is **composable**."